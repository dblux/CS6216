{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import pickle, os\n",
    "import numpy as np\n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import roc_curve, auc, confusion_matrix, classification_report\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Input, Dense, Conv2D, Activation, MaxPool2D, Dropout, Flatten, BatchNormalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processing embeddings\n",
    "EMBEDDING_DIR = \"data/embedding/gae/\"\n",
    "embedding_files = os.listdir(EMBEDDING_DIR)\n",
    "embedding_files.sort()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26598\n",
      "0_1-hsa00140.npy\n",
      "0_10-hsa00140.npy\n",
      "0_100-hsa00140.npy\n",
      "0_101-hsa00140.npy\n",
      "0_102-hsa00140.npy\n",
      "0_103-hsa00140.npy\n",
      "0_104-hsa00140.npy\n",
      "0_105-hsa00140.npy\n",
      "0_106-hsa00140.npy\n",
      "0_107-hsa00140.npy\n",
      "0_108-hsa00140.npy\n",
      "0_109-hsa00140.npy\n",
      "0_11-hsa00140.npy\n",
      "0_110-hsa00140.npy\n",
      "0_111-hsa00140.npy\n",
      "0_112-hsa00140.npy\n",
      "0_113-hsa00140.npy\n",
      "0_114-hsa00140.npy\n",
      "0_115-hsa00140.npy\n",
      "0_116-hsa00140.npy\n",
      "0_117-hsa00140.npy\n",
      "0_118-hsa00140.npy\n",
      "0_119-hsa00140.npy\n",
      "0_12-hsa00140.npy\n",
      "0_120-hsa00140.npy\n",
      "0_121-hsa00140.npy\n",
      "0_122-hsa00140.npy\n",
      "0_123-hsa00140.npy\n",
      "0_124-hsa00140.npy\n",
      "0_125-hsa00140.npy\n",
      "0_126-hsa00140.npy\n",
      "0_127-hsa00140.npy\n",
      "0_128-hsa00140.npy\n",
      "0_129-hsa00140.npy\n",
      "0_13-hsa00140.npy\n",
      "0_130-hsa00140.npy\n",
      "0_131-hsa00140.npy\n",
      "0_132-hsa00140.npy\n",
      "0_133-hsa00140.npy\n",
      "0_134-hsa00140.npy\n",
      "0_135-hsa00140.npy\n",
      "0_136-hsa00140.npy\n",
      "0_137-hsa00140.npy\n",
      "0_138-hsa00140.npy\n",
      "0_139-hsa00140.npy\n",
      "0_14-hsa00140.npy\n",
      "0_140-hsa00140.npy\n",
      "0_141-hsa00140.npy\n",
      "0_142-hsa00140.npy\n",
      "0_143-hsa00140.npy\n",
      "0_144-hsa00140.npy\n",
      "0_145-hsa00140.npy\n",
      "0_146-hsa00140.npy\n",
      "0_147-hsa00140.npy\n",
      "0_148-hsa00140.npy\n",
      "0_149-hsa00140.npy\n",
      "0_15-hsa00140.npy\n",
      "0_150-hsa00140.npy\n",
      "0_151-hsa00140.npy\n",
      "0_152-hsa00140.npy\n",
      "0_153-hsa00140.npy\n",
      "0_154-hsa00140.npy\n",
      "0_155-hsa00140.npy\n",
      "0_156-hsa00140.npy\n",
      "0_157-hsa00140.npy\n",
      "0_158-hsa00140.npy\n",
      "0_159-hsa00140.npy\n",
      "0_16-hsa00140.npy\n",
      "0_160-hsa00140.npy\n",
      "0_161-hsa00140.npy\n",
      "0_162-hsa00140.npy\n",
      "0_163-hsa00140.npy\n",
      "0_164-hsa00140.npy\n",
      "0_165-hsa00140.npy\n",
      "0_166-hsa00140.npy\n",
      "0_167-hsa00140.npy\n",
      "0_168-hsa00140.npy\n",
      "0_169-hsa00140.npy\n",
      "0_17-hsa00140.npy\n",
      "0_170-hsa00140.npy\n",
      "0_171-hsa00140.npy\n",
      "0_172-hsa00140.npy\n",
      "0_173-hsa00140.npy\n",
      "0_174-hsa00140.npy\n",
      "0_175-hsa00140.npy\n",
      "0_176-hsa00140.npy\n",
      "0_177-hsa00140.npy\n",
      "0_178-hsa00140.npy\n",
      "0_179-hsa00140.npy\n",
      "0_18-hsa00140.npy\n",
      "0_19-hsa00140.npy\n",
      "0_2-hsa00140.npy\n",
      "0_20-hsa00140.npy\n",
      "0_21-hsa00140.npy\n",
      "0_22-hsa00140.npy\n",
      "0_23-hsa00140.npy\n",
      "0_24-hsa00140.npy\n",
      "0_25-hsa00140.npy\n",
      "0_26-hsa00140.npy\n",
      "0_27-hsa00140.npy\n",
      "0_28-hsa00140.npy\n",
      "0_29-hsa00140.npy\n",
      "0_3-hsa00140.npy\n",
      "0_30-hsa00140.npy\n",
      "0_31-hsa00140.npy\n",
      "0_32-hsa00140.npy\n",
      "0_33-hsa00140.npy\n",
      "0_34-hsa00140.npy\n",
      "0_35-hsa00140.npy\n",
      "0_36-hsa00140.npy\n",
      "0_37-hsa00140.npy\n",
      "0_38-hsa00140.npy\n",
      "0_39-hsa00140.npy\n",
      "0_4-hsa00140.npy\n",
      "0_40-hsa00140.npy\n",
      "0_41-hsa00140.npy\n",
      "0_42-hsa00140.npy\n",
      "0_43-hsa00140.npy\n",
      "0_44-hsa00140.npy\n",
      "0_45-hsa00140.npy\n",
      "0_46-hsa00140.npy\n",
      "0_47-hsa00140.npy\n",
      "0_48-hsa00140.npy\n",
      "0_49-hsa00140.npy\n",
      "0_5-hsa00140.npy\n",
      "0_50-hsa00140.npy\n",
      "0_51-hsa00140.npy\n",
      "0_52-hsa00140.npy\n",
      "0_53-hsa00140.npy\n",
      "0_54-hsa00140.npy\n",
      "0_55-hsa00140.npy\n",
      "0_56-hsa00140.npy\n",
      "0_57-hsa00140.npy\n",
      "0_58-hsa00140.npy\n",
      "0_59-hsa00140.npy\n",
      "0_6-hsa00140.npy\n",
      "0_60-hsa00140.npy\n",
      "0_61-hsa00140.npy\n",
      "0_62-hsa00140.npy\n",
      "0_63-hsa00140.npy\n",
      "0_64-hsa00140.npy\n",
      "0_65-hsa00140.npy\n",
      "0_66-hsa00140.npy\n",
      "0_67-hsa00140.npy\n",
      "0_68-hsa00140.npy\n",
      "0_69-hsa00140.npy\n",
      "0_7-hsa00140.npy\n",
      "0_70-hsa00140.npy\n",
      "0_71-hsa00140.npy\n",
      "0_72-hsa00140.npy\n",
      "0_73-hsa00140.npy\n",
      "0_74-hsa00140.npy\n",
      "0_75-hsa00140.npy\n",
      "0_76-hsa00140.npy\n",
      "0_77-hsa00140.npy\n",
      "0_78-hsa00140.npy\n",
      "0_79-hsa00140.npy\n",
      "0_8-hsa00140.npy\n",
      "0_80-hsa00140.npy\n",
      "0_81-hsa00140.npy\n",
      "0_82-hsa00140.npy\n",
      "0_83-hsa00140.npy\n",
      "0_84-hsa00140.npy\n",
      "0_85-hsa00140.npy\n",
      "0_86-hsa00140.npy\n",
      "0_87-hsa00140.npy\n",
      "0_88-hsa00140.npy\n",
      "0_89-hsa00140.npy\n",
      "0_9-hsa00140.npy\n",
      "0_90-hsa00140.npy\n",
      "0_91-hsa00140.npy\n",
      "0_92-hsa00140.npy\n",
      "0_93-hsa00140.npy\n",
      "0_94-hsa00140.npy\n",
      "0_95-hsa00140.npy\n",
      "0_96-hsa00140.npy\n",
      "0_97-hsa00140.npy\n",
      "0_98-hsa00140.npy\n",
      "0_99-hsa00140.npy\n",
      "1_1-hsa00140.npy\n",
      "1_10-hsa00140.npy\n",
      "1_100-hsa00140.npy\n",
      "1_101-hsa00140.npy\n",
      "1_102-hsa00140.npy\n",
      "1_103-hsa00140.npy\n",
      "1_104-hsa00140.npy\n",
      "1_105-hsa00140.npy\n",
      "1_106-hsa00140.npy\n",
      "1_107-hsa00140.npy\n",
      "1_11-hsa00140.npy\n",
      "1_12-hsa00140.npy\n",
      "1_13-hsa00140.npy\n",
      "1_14-hsa00140.npy\n",
      "1_15-hsa00140.npy\n",
      "1_16-hsa00140.npy\n",
      "1_17-hsa00140.npy\n",
      "1_18-hsa00140.npy\n",
      "1_19-hsa00140.npy\n",
      "1_2-hsa00140.npy\n",
      "1_20-hsa00140.npy\n",
      "1_21-hsa00140.npy\n",
      "1_22-hsa00140.npy\n",
      "1_23-hsa00140.npy\n",
      "1_24-hsa00140.npy\n",
      "1_25-hsa00140.npy\n",
      "1_26-hsa00140.npy\n",
      "1_27-hsa00140.npy\n",
      "1_28-hsa00140.npy\n",
      "1_29-hsa00140.npy\n",
      "1_3-hsa00140.npy\n",
      "1_30-hsa00140.npy\n",
      "1_31-hsa00140.npy\n",
      "1_32-hsa00140.npy\n",
      "1_33-hsa00140.npy\n",
      "1_34-hsa00140.npy\n",
      "1_35-hsa00140.npy\n",
      "1_36-hsa00140.npy\n",
      "1_37-hsa00140.npy\n",
      "1_38-hsa00140.npy\n",
      "1_39-hsa00140.npy\n",
      "1_4-hsa00140.npy\n",
      "1_40-hsa00140.npy\n",
      "1_41-hsa00140.npy\n",
      "1_42-hsa00140.npy\n",
      "1_43-hsa00140.npy\n",
      "1_44-hsa00140.npy\n",
      "1_45-hsa00140.npy\n",
      "1_46-hsa00140.npy\n",
      "1_47-hsa00140.npy\n",
      "1_48-hsa00140.npy\n",
      "1_49-hsa00140.npy\n",
      "1_5-hsa00140.npy\n",
      "1_50-hsa00140.npy\n",
      "1_51-hsa00140.npy\n",
      "1_52-hsa00140.npy\n",
      "1_53-hsa00140.npy\n",
      "1_54-hsa00140.npy\n",
      "1_55-hsa00140.npy\n",
      "1_56-hsa00140.npy\n",
      "1_57-hsa00140.npy\n",
      "1_58-hsa00140.npy\n",
      "1_59-hsa00140.npy\n",
      "1_6-hsa00140.npy\n",
      "1_60-hsa00140.npy\n",
      "1_61-hsa00140.npy\n",
      "1_62-hsa00140.npy\n",
      "1_63-hsa00140.npy\n",
      "1_64-hsa00140.npy\n",
      "1_65-hsa00140.npy\n",
      "1_66-hsa00140.npy\n",
      "1_67-hsa00140.npy\n",
      "1_68-hsa00140.npy\n",
      "1_69-hsa00140.npy\n",
      "1_7-hsa00140.npy\n",
      "1_70-hsa00140.npy\n",
      "1_71-hsa00140.npy\n",
      "1_72-hsa00140.npy\n",
      "1_73-hsa00140.npy\n",
      "1_74-hsa00140.npy\n",
      "1_75-hsa00140.npy\n",
      "1_76-hsa00140.npy\n",
      "1_77-hsa00140.npy\n",
      "1_78-hsa00140.npy\n",
      "1_79-hsa00140.npy\n",
      "1_8-hsa00140.npy\n",
      "1_80-hsa00140.npy\n",
      "1_81-hsa00140.npy\n",
      "1_82-hsa00140.npy\n",
      "1_83-hsa00140.npy\n",
      "1_84-hsa00140.npy\n",
      "1_85-hsa00140.npy\n",
      "1_86-hsa00140.npy\n",
      "1_87-hsa00140.npy\n",
      "1_88-hsa00140.npy\n",
      "1_89-hsa00140.npy\n",
      "1_9-hsa00140.npy\n",
      "1_90-hsa00140.npy\n",
      "1_91-hsa00140.npy\n",
      "1_92-hsa00140.npy\n",
      "1_93-hsa00140.npy\n",
      "1_94-hsa00140.npy\n",
      "1_95-hsa00140.npy\n",
      "1_96-hsa00140.npy\n",
      "1_97-hsa00140.npy\n",
      "1_98-hsa00140.npy\n",
      "1_99-hsa00140.npy\n"
     ]
    }
   ],
   "source": [
    "num_files = len(embedding_files)\n",
    "print(num_files)\n",
    "NUM_PATHWAY = 93\n",
    "for i in range(286):\n",
    "    print(embedding_files[i*NUM_PATHWAY])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# file = \"0_1-hsa00010.npy\"\n",
    "# id_pathway = np.load(os.path.join(EMBEDDING_DIR, file))\n",
    "# print(id_pathway.shape)\n",
    "# np.sum(id_pathway, axis=0)[np.newaxis,:].shape\n",
    "\n",
    "# Sum of pathways\n",
    "sum_data = [np.sum(np.load(os.path.join(EMBEDDING_DIR, file)), axis=0)[np.newaxis,:] for file in embedding_files]\n",
    "\n",
    "# Flattened data\n",
    "# flattened_data = [np.load(os.path.join(EMBEDDING_DIR, file)).reshape(1,-1) for file in embedding_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Original matrix\n",
    "matrix_data = [np.load(os.path.join(EMBEDDING_DIR, file)) for file in embedding_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(63, 16)\n",
      "26598\n",
      "93\n"
     ]
    }
   ],
   "source": [
    "print(matrix_data[1].shape)\n",
    "print(len(embedding_files))\n",
    "print(NUM_PATHWAY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 93\n",
      "93 186\n",
      "186 279\n",
      "279 372\n",
      "372 465\n",
      "465 558\n",
      "558 651\n",
      "651 744\n",
      "744 837\n",
      "837 930\n",
      "930 1023\n",
      "1023 1116\n",
      "1116 1209\n",
      "1209 1302\n",
      "1302 1395\n",
      "1395 1488\n",
      "1488 1581\n",
      "1581 1674\n",
      "1674 1767\n",
      "1767 1860\n",
      "1860 1953\n",
      "1953 2046\n",
      "2046 2139\n",
      "2139 2232\n",
      "2232 2325\n",
      "2325 2418\n",
      "2418 2511\n",
      "2511 2604\n",
      "2604 2697\n",
      "2697 2790\n",
      "2790 2883\n",
      "2883 2976\n",
      "2976 3069\n",
      "3069 3162\n",
      "3162 3255\n",
      "3255 3348\n",
      "3348 3441\n",
      "3441 3534\n",
      "3534 3627\n",
      "3627 3720\n",
      "3720 3813\n",
      "3813 3906\n",
      "3906 3999\n",
      "3999 4092\n",
      "4092 4185\n",
      "4185 4278\n",
      "4278 4371\n",
      "4371 4464\n",
      "4464 4557\n",
      "4557 4650\n",
      "4650 4743\n",
      "4743 4836\n",
      "4836 4929\n",
      "4929 5022\n",
      "5022 5115\n",
      "5115 5208\n",
      "5208 5301\n",
      "5301 5394\n",
      "5394 5487\n",
      "5487 5580\n",
      "5580 5673\n",
      "5673 5766\n",
      "5766 5859\n",
      "5859 5952\n",
      "5952 6045\n",
      "6045 6138\n",
      "6138 6231\n",
      "6231 6324\n",
      "6324 6417\n",
      "6417 6510\n",
      "6510 6603\n",
      "6603 6696\n",
      "6696 6789\n",
      "6789 6882\n",
      "6882 6975\n",
      "6975 7068\n",
      "7068 7161\n",
      "7161 7254\n",
      "7254 7347\n",
      "7347 7440\n",
      "7440 7533\n",
      "7533 7626\n",
      "7626 7719\n",
      "7719 7812\n",
      "7812 7905\n",
      "7905 7998\n",
      "7998 8091\n",
      "8091 8184\n",
      "8184 8277\n",
      "8277 8370\n",
      "8370 8463\n",
      "8463 8556\n",
      "8556 8649\n",
      "8649 8742\n",
      "8742 8835\n",
      "8835 8928\n",
      "8928 9021\n",
      "9021 9114\n",
      "9114 9207\n",
      "9207 9300\n",
      "9300 9393\n",
      "9393 9486\n",
      "9486 9579\n",
      "9579 9672\n",
      "9672 9765\n",
      "9765 9858\n",
      "9858 9951\n",
      "9951 10044\n",
      "10044 10137\n",
      "10137 10230\n",
      "10230 10323\n",
      "10323 10416\n",
      "10416 10509\n",
      "10509 10602\n",
      "10602 10695\n",
      "10695 10788\n",
      "10788 10881\n",
      "10881 10974\n",
      "10974 11067\n",
      "11067 11160\n",
      "11160 11253\n",
      "11253 11346\n",
      "11346 11439\n",
      "11439 11532\n",
      "11532 11625\n",
      "11625 11718\n",
      "11718 11811\n",
      "11811 11904\n",
      "11904 11997\n",
      "11997 12090\n",
      "12090 12183\n",
      "12183 12276\n",
      "12276 12369\n",
      "12369 12462\n",
      "12462 12555\n",
      "12555 12648\n",
      "12648 12741\n",
      "12741 12834\n",
      "12834 12927\n",
      "12927 13020\n",
      "13020 13113\n",
      "13113 13206\n",
      "13206 13299\n",
      "13299 13392\n",
      "13392 13485\n",
      "13485 13578\n",
      "13578 13671\n",
      "13671 13764\n",
      "13764 13857\n",
      "13857 13950\n",
      "13950 14043\n",
      "14043 14136\n",
      "14136 14229\n",
      "14229 14322\n",
      "14322 14415\n",
      "14415 14508\n",
      "14508 14601\n",
      "14601 14694\n",
      "14694 14787\n",
      "14787 14880\n",
      "14880 14973\n",
      "14973 15066\n",
      "15066 15159\n",
      "15159 15252\n",
      "15252 15345\n",
      "15345 15438\n",
      "15438 15531\n",
      "15531 15624\n",
      "15624 15717\n",
      "15717 15810\n",
      "15810 15903\n",
      "15903 15996\n",
      "15996 16089\n",
      "16089 16182\n",
      "16182 16275\n",
      "16275 16368\n",
      "16368 16461\n",
      "16461 16554\n",
      "16554 16647\n",
      "16647 16740\n",
      "16740 16833\n",
      "16833 16926\n",
      "16926 17019\n",
      "17019 17112\n",
      "17112 17205\n",
      "17205 17298\n",
      "17298 17391\n",
      "17391 17484\n",
      "17484 17577\n",
      "17577 17670\n",
      "17670 17763\n",
      "17763 17856\n",
      "17856 17949\n",
      "17949 18042\n",
      "18042 18135\n",
      "18135 18228\n",
      "18228 18321\n",
      "18321 18414\n",
      "18414 18507\n",
      "18507 18600\n",
      "18600 18693\n",
      "18693 18786\n",
      "18786 18879\n",
      "18879 18972\n",
      "18972 19065\n",
      "19065 19158\n",
      "19158 19251\n",
      "19251 19344\n",
      "19344 19437\n",
      "19437 19530\n",
      "19530 19623\n",
      "19623 19716\n",
      "19716 19809\n",
      "19809 19902\n",
      "19902 19995\n",
      "19995 20088\n",
      "20088 20181\n",
      "20181 20274\n",
      "20274 20367\n",
      "20367 20460\n",
      "20460 20553\n",
      "20553 20646\n",
      "20646 20739\n",
      "20739 20832\n",
      "20832 20925\n",
      "20925 21018\n",
      "21018 21111\n",
      "21111 21204\n",
      "21204 21297\n",
      "21297 21390\n",
      "21390 21483\n",
      "21483 21576\n",
      "21576 21669\n",
      "21669 21762\n",
      "21762 21855\n",
      "21855 21948\n",
      "21948 22041\n",
      "22041 22134\n",
      "22134 22227\n",
      "22227 22320\n",
      "22320 22413\n",
      "22413 22506\n",
      "22506 22599\n",
      "22599 22692\n",
      "22692 22785\n",
      "22785 22878\n",
      "22878 22971\n",
      "22971 23064\n",
      "23064 23157\n",
      "23157 23250\n",
      "23250 23343\n",
      "23343 23436\n",
      "23436 23529\n",
      "23529 23622\n",
      "23622 23715\n",
      "23715 23808\n",
      "23808 23901\n",
      "23901 23994\n",
      "23994 24087\n",
      "24087 24180\n",
      "24180 24273\n",
      "24273 24366\n",
      "24366 24459\n",
      "24459 24552\n",
      "24552 24645\n",
      "24645 24738\n",
      "24738 24831\n",
      "24831 24924\n",
      "24924 25017\n",
      "25017 25110\n",
      "25110 25203\n",
      "25203 25296\n",
      "25296 25389\n",
      "25389 25482\n",
      "25482 25575\n",
      "25575 25668\n",
      "25668 25761\n",
      "25761 25854\n",
      "25854 25947\n",
      "25947 26040\n",
      "26040 26133\n",
      "26133 26226\n",
      "26226 26319\n",
      "26319 26412\n",
      "26412 26505\n",
      "26505 26598\n"
     ]
    }
   ],
   "source": [
    "# Horizontally stack data to form one row per sample\n",
    "data_list = []\n",
    "for indv in range(286):\n",
    "    start_index = indv * NUM_PATHWAY\n",
    "    end_index = (indv + 1) * NUM_PATHWAY\n",
    "    print(start_index, end_index)\n",
    "    # Change from flattened_data\n",
    "    data_list.append(np.hstack(sum_data[start_index:end_index]))\n",
    "print(len(data_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 93\n",
      "93 186\n",
      "186 279\n",
      "279 372\n",
      "372 465\n",
      "465 558\n",
      "558 651\n",
      "651 744\n",
      "744 837\n",
      "837 930\n",
      "930 1023\n",
      "1023 1116\n",
      "1116 1209\n",
      "1209 1302\n",
      "1302 1395\n",
      "1395 1488\n",
      "1488 1581\n",
      "1581 1674\n",
      "1674 1767\n",
      "1767 1860\n",
      "1860 1953\n",
      "1953 2046\n",
      "2046 2139\n",
      "2139 2232\n",
      "2232 2325\n",
      "2325 2418\n",
      "2418 2511\n",
      "2511 2604\n",
      "2604 2697\n",
      "2697 2790\n",
      "2790 2883\n",
      "2883 2976\n",
      "2976 3069\n",
      "3069 3162\n",
      "3162 3255\n",
      "3255 3348\n",
      "3348 3441\n",
      "3441 3534\n",
      "3534 3627\n",
      "3627 3720\n",
      "3720 3813\n",
      "3813 3906\n",
      "3906 3999\n",
      "3999 4092\n",
      "4092 4185\n",
      "4185 4278\n",
      "4278 4371\n",
      "4371 4464\n",
      "4464 4557\n",
      "4557 4650\n",
      "4650 4743\n",
      "4743 4836\n",
      "4836 4929\n",
      "4929 5022\n",
      "5022 5115\n",
      "5115 5208\n",
      "5208 5301\n",
      "5301 5394\n",
      "5394 5487\n",
      "5487 5580\n",
      "5580 5673\n",
      "5673 5766\n",
      "5766 5859\n",
      "5859 5952\n",
      "5952 6045\n",
      "6045 6138\n",
      "6138 6231\n",
      "6231 6324\n",
      "6324 6417\n",
      "6417 6510\n",
      "6510 6603\n",
      "6603 6696\n",
      "6696 6789\n",
      "6789 6882\n",
      "6882 6975\n",
      "6975 7068\n",
      "7068 7161\n",
      "7161 7254\n",
      "7254 7347\n",
      "7347 7440\n",
      "7440 7533\n",
      "7533 7626\n",
      "7626 7719\n",
      "7719 7812\n",
      "7812 7905\n",
      "7905 7998\n",
      "7998 8091\n",
      "8091 8184\n",
      "8184 8277\n",
      "8277 8370\n",
      "8370 8463\n",
      "8463 8556\n",
      "8556 8649\n",
      "8649 8742\n",
      "8742 8835\n",
      "8835 8928\n",
      "8928 9021\n",
      "9021 9114\n",
      "9114 9207\n",
      "9207 9300\n",
      "9300 9393\n",
      "9393 9486\n",
      "9486 9579\n",
      "9579 9672\n",
      "9672 9765\n",
      "9765 9858\n",
      "9858 9951\n",
      "9951 10044\n",
      "10044 10137\n",
      "10137 10230\n",
      "10230 10323\n",
      "10323 10416\n",
      "10416 10509\n",
      "10509 10602\n",
      "10602 10695\n",
      "10695 10788\n",
      "10788 10881\n",
      "10881 10974\n",
      "10974 11067\n",
      "11067 11160\n",
      "11160 11253\n",
      "11253 11346\n",
      "11346 11439\n",
      "11439 11532\n",
      "11532 11625\n",
      "11625 11718\n",
      "11718 11811\n",
      "11811 11904\n",
      "11904 11997\n",
      "11997 12090\n",
      "12090 12183\n",
      "12183 12276\n",
      "12276 12369\n",
      "12369 12462\n",
      "12462 12555\n",
      "12555 12648\n",
      "12648 12741\n",
      "12741 12834\n",
      "12834 12927\n",
      "12927 13020\n",
      "13020 13113\n",
      "13113 13206\n",
      "13206 13299\n",
      "13299 13392\n",
      "13392 13485\n",
      "13485 13578\n",
      "13578 13671\n",
      "13671 13764\n",
      "13764 13857\n",
      "13857 13950\n",
      "13950 14043\n",
      "14043 14136\n",
      "14136 14229\n",
      "14229 14322\n",
      "14322 14415\n",
      "14415 14508\n",
      "14508 14601\n",
      "14601 14694\n",
      "14694 14787\n",
      "14787 14880\n",
      "14880 14973\n",
      "14973 15066\n",
      "15066 15159\n",
      "15159 15252\n",
      "15252 15345\n",
      "15345 15438\n",
      "15438 15531\n",
      "15531 15624\n",
      "15624 15717\n",
      "15717 15810\n",
      "15810 15903\n",
      "15903 15996\n",
      "15996 16089\n",
      "16089 16182\n",
      "16182 16275\n",
      "16275 16368\n",
      "16368 16461\n",
      "16461 16554\n",
      "16554 16647\n",
      "16647 16740\n",
      "16740 16833\n",
      "16833 16926\n",
      "16926 17019\n",
      "17019 17112\n",
      "17112 17205\n",
      "17205 17298\n",
      "17298 17391\n",
      "17391 17484\n",
      "17484 17577\n",
      "17577 17670\n",
      "17670 17763\n",
      "17763 17856\n",
      "17856 17949\n",
      "17949 18042\n",
      "18042 18135\n",
      "18135 18228\n",
      "18228 18321\n",
      "18321 18414\n",
      "18414 18507\n",
      "18507 18600\n",
      "18600 18693\n",
      "18693 18786\n",
      "18786 18879\n",
      "18879 18972\n",
      "18972 19065\n",
      "19065 19158\n",
      "19158 19251\n",
      "19251 19344\n",
      "19344 19437\n",
      "19437 19530\n",
      "19530 19623\n",
      "19623 19716\n",
      "19716 19809\n",
      "19809 19902\n",
      "19902 19995\n",
      "19995 20088\n",
      "20088 20181\n",
      "20181 20274\n",
      "20274 20367\n",
      "20367 20460\n",
      "20460 20553\n",
      "20553 20646\n",
      "20646 20739\n",
      "20739 20832\n",
      "20832 20925\n",
      "20925 21018\n",
      "21018 21111\n",
      "21111 21204\n",
      "21204 21297\n",
      "21297 21390\n",
      "21390 21483\n",
      "21483 21576\n",
      "21576 21669\n",
      "21669 21762\n",
      "21762 21855\n",
      "21855 21948\n",
      "21948 22041\n",
      "22041 22134\n",
      "22134 22227\n",
      "22227 22320\n",
      "22320 22413\n",
      "22413 22506\n",
      "22506 22599\n",
      "22599 22692\n",
      "22692 22785\n",
      "22785 22878\n",
      "22878 22971\n",
      "22971 23064\n",
      "23064 23157\n",
      "23157 23250\n",
      "23250 23343\n",
      "23343 23436\n",
      "23436 23529\n",
      "23529 23622\n",
      "23622 23715\n",
      "23715 23808\n",
      "23808 23901\n",
      "23901 23994\n",
      "23994 24087\n",
      "24087 24180\n",
      "24180 24273\n",
      "24273 24366\n",
      "24366 24459\n",
      "24459 24552\n",
      "24552 24645\n",
      "24645 24738\n",
      "24738 24831\n",
      "24831 24924\n",
      "24924 25017\n",
      "25017 25110\n",
      "25110 25203\n",
      "25203 25296\n",
      "25296 25389\n",
      "25389 25482\n",
      "25482 25575\n",
      "25575 25668\n",
      "25668 25761\n",
      "25761 25854\n",
      "25854 25947\n",
      "25947 26040\n",
      "26040 26133\n",
      "26133 26226\n",
      "26226 26319\n",
      "26319 26412\n",
      "26412 26505\n",
      "26505 26598\n"
     ]
    }
   ],
   "source": [
    "# Form image\n",
    "data_list = []\n",
    "for indv in range(286):\n",
    "    start_index = indv * NUM_PATHWAY\n",
    "    end_index = (indv + 1) * NUM_PATHWAY\n",
    "    print(start_index, end_index)\n",
    "    # Vertically stack data\n",
    "    data_list.append(np.vstack(matrix_data[start_index:end_index]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 5179, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "# Save image\n",
    "X_img_full = np.array(data_list)[:,:,:,np.newaxis]\n",
    "# (n_samples, rows, cols, channels)\n",
    "print(X_img_full.shape)\n",
    "output_fpath = \"data/embedding/X_img_full_93.npy\"\n",
    "np.save(output_fpath, X_img_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save sum\n",
    "X_sum_93 = np.vstack(data_list)\n",
    "print(X_sum_93.shape)\n",
    "# Save processed embedding data (286x104800)\n",
    "# Save X_sum_158.npy (286, 2528)\n",
    "# X_sum_93.npy (286, 1488)\n",
    "output_fpath = \"data/embedding/X_sum_93.npy\"\n",
    "np.save(output_fpath, X_sum_93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5179, 16)\n"
     ]
    }
   ],
   "source": [
    "print(data_list[0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 93, 16, 1)\n",
      "(214, 93, 16, 1)\n"
     ]
    }
   ],
   "source": [
    "# LOAD_DATA = \"data/embedding/X_img_full_93.npy\"\n",
    "LOAD_DATA = \"data/embedding/X_img_sum_93.npy\"\n",
    "# LOAD_DATA = \"data/embedding/X_img_sum_158.npy\"\n",
    "# LOAD_DATA = \"data/embedding/X_embedding.npy\"\n",
    "# Embeddings\n",
    "X = np.load(LOAD_DATA)\n",
    "print(X.shape)\n",
    "\n",
    "# Datasets\n",
    "# X_sum_158 = np.copy(X)\n",
    "# print(X_sum_158.shape)\n",
    "# print(X_sum_93.shape)\n",
    "\n",
    "# Labels y\n",
    "y = np.concatenate((np.repeat(0, 179), np.repeat(1, 107))).astype(\"int16\")\n",
    "\n",
    "# Random stratified split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y,\n",
    "                                                    random_state=1,\n",
    "                                                    test_size=0.25)\n",
    "\n",
    "print(X_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mlp(dim):\n",
    "    vector_input = Input(shape=dim)\n",
    "    h = Dense(5000)(vector_input)\n",
    "    h = Dropout(0.4)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(2000)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(1000)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(500)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(200)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(100)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(50)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(20)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(10)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(8)(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(8)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    h = Activation('relu')(h)\n",
    "    h = Dense(1)(h)\n",
    "    h = BatchNormalization()(h)\n",
    "    output = Activation('sigmoid')(h)\n",
    "    model = Model(inputs=vector_input, outputs=output)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 104800)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 5000)              524005000 \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "activation_1 (Activation)    (None, 5000)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2000)              10002000  \n",
      "_________________________________________________________________\n",
      "activation_2 (Activation)    (None, 2000)              0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 1000)              2001000   \n",
      "_________________________________________________________________\n",
      "activation_3 (Activation)    (None, 1000)              0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 500)               500500    \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 200)               100200    \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 100)               20100     \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 100)               0         \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 50)                5050      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 50)                0         \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 20)                1020      \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 20)                0         \n",
      "_________________________________________________________________\n",
      "dense_9 (Dense)              (None, 10)                210       \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 10)                0         \n",
      "_________________________________________________________________\n",
      "dense_10 (Dense)             (None, 8)                 88        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_11 (Dense)             (None, 8)                 72        \n",
      "_________________________________________________________________\n",
      "batch_normalization_1 (Batch (None, 8)                 32        \n",
      "_________________________________________________________________\n",
      "activation_11 (Activation)   (None, 8)                 0         \n",
      "_________________________________________________________________\n",
      "dense_12 (Dense)             (None, 1)                 9         \n",
      "_________________________________________________________________\n",
      "batch_normalization_2 (Batch (None, 1)                 4         \n",
      "_________________________________________________________________\n",
      "activation_12 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 536,635,285\n",
      "Trainable params: 536,635,267\n",
      "Non-trainable params: 18\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training model...\n",
      "Train on 214 samples, validate on 72 samples\n",
      "Epoch 1/30\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "n_epoch = 30\n",
    "\n",
    "input_dim = (X_train.shape[1],)\n",
    "mlp = create_mlp(dim=input_dim)\n",
    "print(mlp.summary())\n",
    "\n",
    "mlp.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# 学習を開始します。\n",
    "print('Training model...')\n",
    "mlp_hist = mlp.fit(X_train, y_train,\n",
    "                   epochs=n_epoch,\n",
    "                   verbose=1,\n",
    "                   validation_data=(X_test, y_test),\n",
    "                   batch_size=batch_size)\n",
    "\n",
    "# with open('../fig/mlp_history.pkl', 'wb') as file:\n",
    "#     pickle.dump(mlp_hist.history, file)\n",
    "\n",
    "# ## 学習結果の確認\n",
    "print('MLP Validation Loss:', mlp_hist.history['loss'][-1])\n",
    "print('MLP Validation Accuracy:', mlp_hist.history['acc'][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_cnn(dim=(5179,16,1)):\n",
    "    model = Sequential()\n",
    "    \n",
    "    model.add(Conv2D(8, input_shape=dim,\n",
    "                     kernel_size=(7, 3),\n",
    "                     strides=(1, 1),\n",
    "                     padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    \n",
    "    model.add(Conv2D(8,\n",
    "                     kernel_size=(7, 3),\n",
    "                     strides=(1, 1),\n",
    "                     padding='same'))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "   \n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(64))\n",
    "    model.add(Activation('relu'))\n",
    "    model.add(BatchNormalization())\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1))\n",
    "    model.add(Activation('sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_3 (Conv2D)            (None, 93, 16, 8)         176       \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 93, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_5 (Batch (None, 93, 16, 8)         32        \n",
      "_________________________________________________________________\n",
      "dropout_5 (Dropout)          (None, 93, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 93, 16, 8)         1352      \n",
      "_________________________________________________________________\n",
      "activation_7 (Activation)    (None, 93, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_6 (Batch (None, 93, 16, 8)         32        \n",
      "_________________________________________________________________\n",
      "dropout_6 (Dropout)          (None, 93, 16, 8)         0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 11904)             0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 64)                761920    \n",
      "_________________________________________________________________\n",
      "activation_8 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_7 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_7 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_5 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "activation_9 (Activation)    (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "batch_normalization_8 (Batch (None, 64)                256       \n",
      "_________________________________________________________________\n",
      "dropout_8 (Dropout)          (None, 64)                0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 1)                 65        \n",
      "_________________________________________________________________\n",
      "activation_10 (Activation)   (None, 1)                 0         \n",
      "=================================================================\n",
      "Total params: 768,249\n",
      "Trainable params: 767,961\n",
      "Non-trainable params: 288\n",
      "_________________________________________________________________\n",
      "None\n",
      "Training model...\n",
      "Train on 214 samples, validate on 72 samples\n",
      "Epoch 1/30\n",
      "214/214 [==============================] - 2s 9ms/step - loss: 1.2158 - acc: 0.5000 - val_loss: 0.7845 - val_acc: 0.4167\n",
      "Epoch 2/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.9850 - acc: 0.5607 - val_loss: 0.6991 - val_acc: 0.5417\n",
      "Epoch 3/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6934 - acc: 0.6542 - val_loss: 0.6725 - val_acc: 0.5694\n",
      "Epoch 4/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.7245 - acc: 0.6075 - val_loss: 0.7077 - val_acc: 0.6111\n",
      "Epoch 5/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.8558 - acc: 0.5841 - val_loss: 0.7044 - val_acc: 0.5556\n",
      "Epoch 6/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6465 - acc: 0.6682 - val_loss: 0.7064 - val_acc: 0.5972\n",
      "Epoch 7/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.6329 - acc: 0.6916 - val_loss: 0.7003 - val_acc: 0.5972\n",
      "Epoch 8/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.5940 - acc: 0.7056 - val_loss: 0.7055 - val_acc: 0.5972\n",
      "Epoch 9/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.5739 - acc: 0.7243 - val_loss: 0.7274 - val_acc: 0.5833\n",
      "Epoch 10/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.5802 - acc: 0.7056 - val_loss: 0.7593 - val_acc: 0.5833\n",
      "Epoch 11/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.4575 - acc: 0.8037 - val_loss: 0.7379 - val_acc: 0.5972\n",
      "Epoch 12/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.5268 - acc: 0.7664 - val_loss: 0.7262 - val_acc: 0.5833\n",
      "Epoch 13/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.4748 - acc: 0.7991 - val_loss: 0.7145 - val_acc: 0.5556\n",
      "Epoch 14/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.4405 - acc: 0.7710 - val_loss: 0.7318 - val_acc: 0.5417\n",
      "Epoch 15/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.4324 - acc: 0.7944 - val_loss: 0.7319 - val_acc: 0.5694\n",
      "Epoch 16/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.3827 - acc: 0.8318 - val_loss: 0.7383 - val_acc: 0.5833\n",
      "Epoch 17/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.3213 - acc: 0.8505 - val_loss: 0.7354 - val_acc: 0.5833\n",
      "Epoch 18/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.3802 - acc: 0.8458 - val_loss: 0.7521 - val_acc: 0.5833\n",
      "Epoch 19/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.3592 - acc: 0.8551 - val_loss: 0.7621 - val_acc: 0.5278\n",
      "Epoch 20/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.3895 - acc: 0.8224 - val_loss: 0.7610 - val_acc: 0.5694\n",
      "Epoch 21/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.2965 - acc: 0.8411 - val_loss: 0.7683 - val_acc: 0.5556\n",
      "Epoch 22/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.4051 - acc: 0.8224 - val_loss: 0.7433 - val_acc: 0.5833\n",
      "Epoch 23/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.2400 - acc: 0.8972 - val_loss: 0.7466 - val_acc: 0.5833\n",
      "Epoch 24/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.2181 - acc: 0.9019 - val_loss: 0.7497 - val_acc: 0.5833\n",
      "Epoch 25/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.2901 - acc: 0.9019 - val_loss: 0.7605 - val_acc: 0.6111\n",
      "Epoch 26/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.2688 - acc: 0.8645 - val_loss: 0.7751 - val_acc: 0.5694\n",
      "Epoch 27/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.2237 - acc: 0.9159 - val_loss: 0.7965 - val_acc: 0.5556\n",
      "Epoch 28/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.1482 - acc: 0.9439 - val_loss: 0.8074 - val_acc: 0.5694\n",
      "Epoch 29/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.2267 - acc: 0.9252 - val_loss: 0.8152 - val_acc: 0.5417\n",
      "Epoch 30/30\n",
      "214/214 [==============================] - 1s 3ms/step - loss: 0.2403 - acc: 0.9159 - val_loss: 0.8309 - val_acc: 0.5139\n"
     ]
    }
   ],
   "source": [
    "batch_size = 10\n",
    "n_epoch = 30\n",
    "\n",
    "img_dim = X_train.shape\n",
    "cnn = create_cnn(dim=img_dim[1:4])\n",
    "print(cnn.summary())\n",
    "\n",
    "cnn.compile(loss='binary_crossentropy',\n",
    "            optimizer='adam',\n",
    "            metrics=['accuracy'])\n",
    "\n",
    "# 学習を開始します。\n",
    "print('Training model...')\n",
    "cnn_hist = cnn.fit(X_train, y_train,\n",
    "                   epochs=n_epoch,\n",
    "                   verbose=1,\n",
    "                   validation_data=(X_test, y_test),\n",
    "                   batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Loss: 0.8309401240613725\n",
      "CNN Accuracy: 0.513888900478681\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.77      0.66        44\n",
      "           1       0.23      0.11      0.15        28\n",
      "\n",
      "   micro avg       0.51      0.51      0.51        72\n",
      "   macro avg       0.40      0.44      0.40        72\n",
      "weighted avg       0.44      0.51      0.46        72\n",
      "\n",
      "[[34 10]\n",
      " [25  3]]\n"
     ]
    }
   ],
   "source": [
    "with open('dump/cnn_history-graph.pkl', 'wb') as file:\n",
    "    pickle.dump(cnn_hist.history, file)\n",
    "\n",
    "## 学習結果の確認\n",
    "print('CNN Loss:', cnn_hist.history[\"val_loss\"][-1])\n",
    "print('CNN Accuracy:', cnn_hist.history[\"val_acc\"][-1])\n",
    "\n",
    "# Keras: Evaluation\n",
    "y_proba = cnn.predict(X_test, batch_size)\n",
    "y_predicted = np.array([1 if y >= 0.5 else 0 for y in y_proba])[:,np.newaxis]\n",
    "\n",
    "print(classification_report(y_test, y_predicted))\n",
    "print(confusion_matrix(y_test, y_predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dump performance metrics\n",
    "with open('dump/logreg_linear_smallmlp.pkl', 'wb') as file:\n",
    "    pickle.dump(performance, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.3333333333333333, 0.4444444444444444, 0.5833333333333334, 0.6388888888888888, 0.4861111111111111]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.42      0.23      0.29        44\n",
      "           1       0.29      0.50      0.37        28\n",
      "\n",
      "   micro avg       0.33      0.33      0.33        72\n",
      "   macro avg       0.35      0.36      0.33        72\n",
      "weighted avg       0.37      0.33      0.32        72\n",
      "\n",
      "[[10 34]\n",
      " [14 14]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.55      0.50      0.52        44\n",
      "           1       0.31      0.36      0.33        28\n",
      "\n",
      "   micro avg       0.44      0.44      0.44        72\n",
      "   macro avg       0.43      0.43      0.43        72\n",
      "weighted avg       0.46      0.44      0.45        72\n",
      "\n",
      "[[22 22]\n",
      " [18 10]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.60      0.95      0.74        44\n",
      "           1       0.00      0.00      0.00        28\n",
      "\n",
      "   micro avg       0.58      0.58      0.58        72\n",
      "   macro avg       0.30      0.48      0.37        72\n",
      "weighted avg       0.37      0.58      0.45        72\n",
      "\n",
      "[[42  2]\n",
      " [28  0]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.64      0.95      0.76        44\n",
      "           1       0.67      0.14      0.24        28\n",
      "\n",
      "   micro avg       0.64      0.64      0.64        72\n",
      "   macro avg       0.65      0.55      0.50        72\n",
      "weighted avg       0.65      0.64      0.56        72\n",
      "\n",
      "[[42  2]\n",
      " [24  4]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.58      0.59      0.58        44\n",
      "           1       0.33      0.32      0.33        28\n",
      "\n",
      "   micro avg       0.49      0.49      0.49        72\n",
      "   macro avg       0.46      0.46      0.46        72\n",
      "weighted avg       0.48      0.49      0.48        72\n",
      "\n",
      "[[26 18]\n",
      " [19  9]]\n"
     ]
    }
   ],
   "source": [
    "acc, y_prob, y_predicted, y_test = performance\n",
    "print(acc)\n",
    "\n",
    "for y_i in y_predicted:\n",
    "    print(classification_report(y_test, y_i))\n",
    "    print(confusion_matrix(y_test, y_i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(286, 104800)\n",
      "(286, 250)\n"
     ]
    }
   ],
   "source": [
    "# PCA\n",
    "EMBEDDING_FILE = \"data/embedding/X_embedding.npy\"\n",
    "X = np.load(EMBEDDING_FILE)\n",
    "print(X.shape)\n",
    "X_reduced = PCA(n_components=250).fit_transform(X)\n",
    "\n",
    "# Save top 1000 principal components\n",
    "output_fpath = \"data/embedding/X_reduced.npy\"\n",
    "np.save(output_fpath, X_reduced)\n",
    "print(X_reduced.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 4, 3, 1)"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.arange(12).reshape(4,3)\n",
    "ls = [a,a,a,a,a,a]\n",
    "np.array(ls)[:,:,:,np.newaxis].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
